# Microbenchmarking Rules and Best Practices for `/benchmarks`

This document details the rules, conventions, and best practices for writing microbenchmarks in this repository. Follow these guidelines to ensure consistency, accuracy, and usefulness of all benchmarks.

______________________________________________________________________

## 1. General Principles

- **Benchmark both reference and optimized implementations**: For every function, write a `test_xxx` for the reference (`eth_abi.*`) and a `test_faster_xxx` for the optimized (`faster_eth_abi.*`) version.
- **Use real, production-like data**: Always use realistic, mainnet-style data (e.g., real ABIs, transaction dicts, logs, blocks, topics). Avoid synthetic or trivial examples unless testing a specific edge case.
- **No stubs or simulation comments**: Do not use comments like "simulate a large ABI" or placeholder data. Always use actual, full data structures.
- **No redundant parameterizations**: Do not parametrize with both synthetic and real data for the same function. Use only the most representative, production-like cases.

______________________________________________________________________

## 2. Parameterization and Coverage

- **Parameterize to cover all meaningful code paths**: For each function, analyze its internal logic and dependencies. Ensure parameterization covers:
  - All input types the function is expected to handle (e.g., list, dict, string, empty, mixed).
  - All meaningful branches and edge cases (e.g., empty arrays, missing keys, deeply nested structures).
  - For functions that raise, include error-triggering cases and use the specific expected exception type.
- **Use descriptive `ids` for all parameterized cases**: Every `@pytest.mark.parametrize` must include an `ids` argument with clear, human-readable scenario names (e.g., `"real_topics"`, `"empty_list"`, `"dict_with_removal"`).
- **Remove redundant or invalid cases**: Only include parameterizations that are valid and exercise a real code path.
- **No use of `pytest.param(..., id=...)` inside the value list**: Use a plain list of values and a separate `ids` list for clarity and consistency. You can make exceptions to this rule if is allows us to write less verbose code.

______________________________________________________________________

## 3. Benchmark Structure

- **One `benchmark` call per test function**: Do not call `benchmark` multiple times in a single test. Use parameterization for multiple scenarios.
- **Use helpers for loops**: Use `run_10`, `run_100`, `run_500`, or `run_1000` (as appropriate for function speed) to loop the function under test. For error cases, use `run_10_exc`, `run_100_exc`, etc., always specifying the expected exception type.
- **Use a pure no-op formatter for infrastructure benchmarks**: When benchmarking formatting infrastructure, use:
  ```python
  def noop(x):
      return x
  ```
  This ensures you are measuring only the overhead of the infrastructure, not the cost of any real transformation.

______________________________________________________________________

## 4. Imports and Access Patterns

- **Import modules, not individual functions**: Use `import eth_abi.registry` and `import faster_eth_abi.registry`, not `from ... import ...`.
- **Access functions and exceptions via module attributes**: E.g., `eth_abi.registry.function_name`, `faster_eth_abi.exceptions.ValueOutOfBounds`.
- **Match the style used in `benchmarks/eth_abi/test_abi_benchmarks.py` and other canonical files.**

______________________________________________________________________

## 5. Mocking, Patching, and DRY

- **Centralize shared mocking logic**: Use helper files (e.g., `fake_rpc.py`) to DRY out repeated logic for mocking network or async code.
- **Use `unittest.mock.patch` for network/async code**: Patch only the external library (e.g., `requests`, `aiohttp`), not the code under test.
- **Import and use shared parameter sets**: Common parameter sets (e.g., names, labels, addresses) should be defined in a shared file (e.g., `params.py`) and imported into each test file.

______________________________________________________________________

## 6. Comments and Documentation

- **Add comments above each parameterization block**: Explain what each case is testing and why it is relevant for real-world usage or code path coverage.
- **Document any edge cases or rationale for parameter choices.**
- **Group names and ids**: Always choose group names and ids that make benchmark results easy to interpret.

______________________________________________________________________

## 7. Error Handling

- **For error cases, always specify the expected exception type** in the error helper (e.g., `run_10_exc(Web3TypeError, ...)`).
- **Include error cases only when they are part of expected usage**.

______________________________________________________________________

## 8. Async and Patching Patterns

- **Async benchmarks**: Use async helpers and patching as needed, following the patterns in async benchmark files.
- **Patch only the external library**: Never patch the code under test directly.

______________________________________________________________________

## 9. Consistency and Naming

- **Consistent naming**: Test function names, parameter names, and ids should be clear and descriptive.
- **Paired benchmarks**: Always provide both reference and optimized benchmarks for every function.

______________________________________________________________________

## 10. Shared Parameterization Patterns

- **Shared parameterization in a separate file**: Only use a shared parameterization decorator or variable in a separate file if it is reused across multiple test modules. For example, `TYPE_STRINGS` in `benchmarks/type_strings.py` is used in most of our benchmarks.
- **Otherwise, keep shared parameterizations local**: If a parameterization is only used in a single test file, define it in that file for clarity and maintainability.

______________________________________________________________________

**Follow these rules for all new and updated benchmarks in this repository.**
